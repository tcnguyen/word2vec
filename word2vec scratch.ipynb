{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"./pride_and_prejudice.txt\", \"r\") as f:\n",
    "    corpus = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = corpus[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_text(corpus):\n",
    "    # lowercase\n",
    "    corpus = sum([a.lower().split() for a in corpus if a != \"\\n\"],[])\n",
    "    corpus = [re.sub('[^A-Za-z0-9]+', '', token) for token in corpus]\n",
    "    corpus = [token for token in corpus if len(token) > 0] \n",
    "    corpus = [re.sub('^\\d+$', '_NUM_', token) for token in corpus]\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "corpus = preprocess_text(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124543"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  7044\n",
      "['13420txt', '13420zip', '15th', '18th', '1a', '1b', '1c', '1d', '1e', '1e1', '1e2', '1e3', '1e4', '1e5', '1e6', '1e7', '1e8', '1e9', '1f', '1f1', '1f2', '1f3', '1f4', '1f5', '1f6', '26th', '501c3', '_NUM_', 'a', 'abatement', 'abhorrence', 'abhorrent', 'abide', 'abiding', 'abilities', 'able', 'ablution', 'abode', 'abominable', 'abominably', 'abominate', 'abound', 'about', 'above', 'abroad', 'abrupt', 'abruptly', 'abruptness', 'absence', 'absent', 'absolute', 'absolutely', 'absurd', 'absurdities', 'absurdity', 'abundant', 'abundantly', 'abuse', 'abused', 'abusing', 'abusive', 'accede', 'acceded', 'acceding', 'accent', 'accents', 'accept', 'acceptable', 'acceptance', 'accepted', 'accepting', 'access', 'accessed', 'accessible', 'accident', 'accidental', 'accidentally', 'accompanied', 'accompany', 'accompanying', 'accomplished', 'accomplishedshe', 'accomplishment', 'accomplishments', 'accordance', 'according', 'accordingly', 'accosted', 'account', 'accounted', 'accounting', 'accounts', 'accuracy', 'accurate', 'accusation', 'accusations', 'accuse', 'accused', 'accusing', 'accustomed']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(corpus)))\n",
    "print( \"Vocabulary size: \", len(vocab))\n",
    "print(vocab[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Total words: 125000\n",
    "- Vocabulary size: 7100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word to id representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_to_ids = {vocab[i]:(i+1) for i in range(len(vocab))}\n",
    "id_to_words = {(i+1):vocab[i] for i in range(len(vocab))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6267, 4911, 2887, 2025, 4332, 4852, 319, 4797, 859, 3545, 548, 6302, 2025, 3532, 2574, 6267, 6655, 4332, 365, 367, 504, 4222, 1409, 319, 6937, 269, 4222, 5367, 6867, 7026, 3931, 1390, 3534, 2754, 3534, 572, 4388, 5390, 3534, 6516, 6267, 6251, 4332, 6267, 4911, 2887, 3735, 3274, 6937, 6302, 2025, 4388, 4369, 504, 7002, 6347, 4852, 319, 4797, 550, 3545, 548, 4756, 1521, 545, 28, 28, 2025, 28, 5213, 1521, 3587, 28, 3662, 6645, 4326, 28, 28, 3655, 2146, 974, 5658, 2114, 6665, 5953, 4332, 6302, 4911, 2887, 2025, 4852, 319, 4797, 4889, 859, 344, 6757, 4852, 319, 4797]\n"
     ]
    }
   ],
   "source": [
    "word_ids = [word_to_ids[token] for token in corpus]\n",
    "print(word_ids[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_word_pair(word_ids, C):\n",
    "    # cut corpus in batch_size\n",
    "    N = len(word_ids)    \n",
    "    M = (N-2*C) * 2*C\n",
    "    centers = [0] * M\n",
    "    targets = [0] * M\n",
    "    \n",
    "    for i in range(C, N-C):        \n",
    "        k = (i-C)*2*C + C\n",
    "        \n",
    "        for j in range(1, C + 1):            \n",
    "            centers[k - j]  = word_ids[i]\n",
    "            targets[k - j]  = word_ids[i - j]            \n",
    "            \n",
    "            centers[k + j - 1]  = word_ids[i]\n",
    "            targets[k + j - 1]  = word_ids[i + j]\n",
    "            \n",
    "    return list(zip(centers, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 1), (3, 2), (3, 4), (3, 5), (4, 2), (4, 3), (4, 5), (4, 6), (5, 3), (5, 4), (5, 6), (5, 7), (6, 4), (6, 5), (6, 7), (6, 8)]\n"
     ]
    }
   ],
   "source": [
    "example_pairs = create_word_pair([1,2,3,4,5,6,7,8], 2)\n",
    "print(example_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random \n",
    "def create_batches(word_pairs, batch_size):\n",
    "    random.shuffle(word_pairs)\n",
    "    M = len(word_pairs) // batch_size\n",
    "    if len(word_pairs) > batch_size * M:\n",
    "        M += 1\n",
    "    \n",
    "    return [word_pairs[i*batch_size:(i+1)*batch_size] for i in range(M)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(5, 4), (4, 2), (6, 7), (5, 3)],\n",
       " [(6, 5), (4, 6), (3, 4), (6, 4)],\n",
       " [(6, 8), (4, 5), (4, 3), (3, 5)],\n",
       " [(3, 2), (5, 6), (3, 1), (5, 7)]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_batches(example_pairs, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 4, 6, 5), (4, 2, 7, 3)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(*[(5, 4), (4, 2), (6, 7), (5, 3)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "- Input : batch of (id_word, id_context_word)\n",
    "- 2 embedded matrix each of size (VxD): P and Q\n",
    "-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary_size = len(vocab) + 1 # since our word id start from 1, normally there will be unknown word\n",
    "context_size = 3 # context size \n",
    "embedding_size = 20\n",
    "batch_size = 128\n",
    "\n",
    "embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0) )\n",
    "\n",
    "nce_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "\n",
    "# placeholder for input and output\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "\n",
    "train_embeddings = tf.nn.embedding_lookup(params=embeddings, ids=train_inputs)\n",
    "\n",
    "train_input_vectors = tf.nn.embedding_lookup(embeddings, train_inputs) # [batch_size, embedding_size]\n",
    "loss = tf.nn.nce_loss(\n",
    "    weights = nce_weights,  #A Tensor of shape [num_classes, dim]. The (possibly-partitioned) class embeddings.\n",
    "    biases = nce_biases, #biases: A Tensor of shape [num_classes]. The class biases.\n",
    "    labels = train_labels, # A Tensor of type int64 and shape [batch_size, num_true]. The target classes.\n",
    "    inputs = train_embeddings, # A Tensor of shape [batch_size, dim]. The forward activations of the input network.\n",
    "    num_sampled = 5,\n",
    "    num_classes = vocabulary_size, # An int. The number of possible classes.\n",
    "    num_true=1,  # An int. The number of target classes per training example.\n",
    "    sampled_values=None,\n",
    "    remove_accidental_hits=False,\n",
    "    partition_strategy='mod',\n",
    "    name='nce_loss'\n",
    ")\n",
    " \n",
    "loss = tf.reduce_mean(loss)\n",
    "    \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_word_ids = word_ids[:1000]\n",
    "sample_pairs = create_word_pair(sample_word_ids, context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_pairs = create_word_pair(word_ids, context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 , batch_number:  1000 , batch loss:  20.3317\n",
      "epoch:  0 , batch_number:  2000 , batch loss:  30.6126\n",
      "epoch:  0 , batch_number:  3000 , batch loss:  16.7792\n",
      "epoch:  0 , batch_number:  4000 , batch loss:  21.4576\n",
      "epoch:  0 , batch_number:  5000 , batch loss:  1.03154\n",
      "epoch:  1 , batch_number:  1000 , batch loss:  36.5153\n",
      "epoch:  1 , batch_number:  2000 , batch loss:  8.45913\n",
      "epoch:  1 , batch_number:  3000 , batch loss:  4.92724\n",
      "epoch:  1 , batch_number:  4000 , batch loss:  24.2695\n",
      "epoch:  1 , batch_number:  5000 , batch loss:  8.09044\n",
      "epoch:  2 , batch_number:  1000 , batch loss:  25.4754\n",
      "epoch:  2 , batch_number:  2000 , batch loss:  7.99338\n",
      "epoch:  2 , batch_number:  3000 , batch loss:  4.97688\n",
      "epoch:  2 , batch_number:  4000 , batch loss:  11.0008\n",
      "epoch:  2 , batch_number:  5000 , batch loss:  0.295123\n",
      "epoch:  3 , batch_number:  1000 , batch loss:  10.1689\n",
      "epoch:  3 , batch_number:  2000 , batch loss:  1.20438\n",
      "epoch:  3 , batch_number:  3000 , batch loss:  11.2117\n",
      "epoch:  3 , batch_number:  4000 , batch loss:  11.0137\n",
      "epoch:  3 , batch_number:  5000 , batch loss:  2.10805\n",
      "epoch:  4 , batch_number:  1000 , batch loss:  26.8929\n",
      "epoch:  4 , batch_number:  2000 , batch loss:  1.24088\n",
      "epoch:  4 , batch_number:  3000 , batch loss:  9.9335\n",
      "epoch:  4 , batch_number:  4000 , batch loss:  0.254826\n",
      "epoch:  4 , batch_number:  5000 , batch loss:  0.72143\n",
      "epoch:  5 , batch_number:  1000 , batch loss:  0.716555\n",
      "epoch:  5 , batch_number:  2000 , batch loss:  3.30428\n",
      "epoch:  5 , batch_number:  3000 , batch loss:  1.76337\n",
      "epoch:  5 , batch_number:  4000 , batch loss:  12.9617\n",
      "epoch:  5 , batch_number:  5000 , batch loss:  8.03016\n",
      "epoch:  6 , batch_number:  1000 , batch loss:  15.5815\n",
      "epoch:  6 , batch_number:  2000 , batch loss:  21.1634\n",
      "epoch:  6 , batch_number:  3000 , batch loss:  4.72372\n",
      "epoch:  6 , batch_number:  4000 , batch loss:  0.447769\n",
      "epoch:  6 , batch_number:  5000 , batch loss:  19.5245\n",
      "epoch:  7 , batch_number:  1000 , batch loss:  0.276166\n",
      "epoch:  7 , batch_number:  2000 , batch loss:  0.990127\n",
      "epoch:  7 , batch_number:  3000 , batch loss:  7.92609\n",
      "epoch:  7 , batch_number:  4000 , batch loss:  1.74128\n",
      "epoch:  7 , batch_number:  5000 , batch loss:  0.749614\n",
      "epoch:  8 , batch_number:  1000 , batch loss:  0.342922\n",
      "epoch:  8 , batch_number:  2000 , batch loss:  0.625097\n",
      "epoch:  8 , batch_number:  3000 , batch loss:  11.4823\n",
      "epoch:  8 , batch_number:  4000 , batch loss:  2.73714\n",
      "epoch:  8 , batch_number:  5000 , batch loss:  0.190966\n",
      "epoch:  9 , batch_number:  1000 , batch loss:  0.244794\n",
      "epoch:  9 , batch_number:  2000 , batch loss:  0.549561\n",
      "epoch:  9 , batch_number:  3000 , batch loss:  3.24936\n",
      "epoch:  9 , batch_number:  4000 , batch loss:  0.29053\n",
      "epoch:  9 , batch_number:  5000 , batch loss:  0.324324\n",
      "epoch:  10 , batch_number:  1000 , batch loss:  0.58412\n",
      "epoch:  10 , batch_number:  2000 , batch loss:  0.443232\n",
      "epoch:  10 , batch_number:  3000 , batch loss:  0.401706\n",
      "epoch:  10 , batch_number:  4000 , batch loss:  11.564\n",
      "epoch:  10 , batch_number:  5000 , batch loss:  12.5664\n",
      "epoch:  11 , batch_number:  1000 , batch loss:  0.352071\n",
      "epoch:  11 , batch_number:  2000 , batch loss:  1.16394\n",
      "epoch:  11 , batch_number:  3000 , batch loss:  0.439685\n",
      "epoch:  11 , batch_number:  4000 , batch loss:  2.27223\n",
      "epoch:  11 , batch_number:  5000 , batch loss:  0.407999\n",
      "epoch:  12 , batch_number:  1000 , batch loss:  0.763408\n",
      "epoch:  12 , batch_number:  2000 , batch loss:  3.37406\n",
      "epoch:  12 , batch_number:  3000 , batch loss:  12.445\n",
      "epoch:  12 , batch_number:  4000 , batch loss:  21.8964\n",
      "epoch:  12 , batch_number:  5000 , batch loss:  9.63243\n",
      "epoch:  13 , batch_number:  1000 , batch loss:  1.06366\n",
      "epoch:  13 , batch_number:  2000 , batch loss:  3.53947\n",
      "epoch:  13 , batch_number:  3000 , batch loss:  10.1115\n",
      "epoch:  13 , batch_number:  4000 , batch loss:  0.319703\n",
      "epoch:  13 , batch_number:  5000 , batch loss:  0.415086\n",
      "epoch:  14 , batch_number:  1000 , batch loss:  0.465268\n",
      "epoch:  14 , batch_number:  2000 , batch loss:  0.246343\n",
      "epoch:  14 , batch_number:  3000 , batch loss:  0.379394\n",
      "epoch:  14 , batch_number:  4000 , batch loss:  8.9321\n",
      "epoch:  14 , batch_number:  5000 , batch loss:  0.417839\n",
      "epoch:  15 , batch_number:  1000 , batch loss:  0.489538\n",
      "epoch:  15 , batch_number:  2000 , batch loss:  0.857375\n",
      "epoch:  15 , batch_number:  3000 , batch loss:  0.986387\n",
      "epoch:  15 , batch_number:  4000 , batch loss:  0.608725\n",
      "epoch:  15 , batch_number:  5000 , batch loss:  1.17502\n",
      "epoch:  16 , batch_number:  1000 , batch loss:  0.326908\n",
      "epoch:  16 , batch_number:  2000 , batch loss:  0.238299\n",
      "epoch:  16 , batch_number:  3000 , batch loss:  0.847858\n",
      "epoch:  16 , batch_number:  4000 , batch loss:  1.10448\n",
      "epoch:  16 , batch_number:  5000 , batch loss:  0.476177\n",
      "epoch:  17 , batch_number:  1000 , batch loss:  1.35813\n",
      "epoch:  17 , batch_number:  2000 , batch loss:  0.67794\n",
      "epoch:  17 , batch_number:  3000 , batch loss:  3.83223\n",
      "epoch:  17 , batch_number:  4000 , batch loss:  3.93456\n",
      "epoch:  17 , batch_number:  5000 , batch loss:  4.24931\n",
      "epoch:  18 , batch_number:  1000 , batch loss:  7.38322\n",
      "epoch:  18 , batch_number:  2000 , batch loss:  11.6518\n",
      "epoch:  18 , batch_number:  3000 , batch loss:  1.29164\n",
      "epoch:  18 , batch_number:  4000 , batch loss:  4.96181\n",
      "epoch:  18 , batch_number:  5000 , batch loss:  0.597068\n",
      "epoch:  19 , batch_number:  1000 , batch loss:  0.451513\n",
      "epoch:  19 , batch_number:  2000 , batch loss:  0.815517\n",
      "epoch:  19 , batch_number:  3000 , batch loss:  2.26404\n",
      "epoch:  19 , batch_number:  4000 , batch loss:  7.10317\n",
      "epoch:  19 , batch_number:  5000 , batch loss:  0.596293\n",
      "Model saved in path: /tmp/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 20\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epoch):\n",
    "        batches = create_batches(batch_size=batch_size, word_pairs=word_pairs)\n",
    "        for (i, batch) in enumerate(batches):\n",
    "            batch_inputs, batch_labels = list(zip(*batch))\n",
    "            \n",
    "            #resaphe to [batch_size, 1] dimension\n",
    "            batch_labels = np.expand_dims(batch_labels, axis=1) \n",
    "            \n",
    "            sess.run(optimizer, feed_dict={train_inputs:batch_inputs, train_labels:batch_labels})\n",
    "            if (i+1) % 1000 == 0:\n",
    "                batch_loss = sess.run(loss,  feed_dict={train_inputs:batch_inputs, train_labels:batch_labels})\n",
    "                print(\"epoch: \", epoch, \", batch_number: \", i+1, \", batch loss: \", batch_loss)\n",
    "        \n",
    "    word_embeddings = sess.run(embeddings)\n",
    "    save_path = saver.save(sess, \"/tmp/model.ckpt\")\n",
    "    print(\"Model saved in path: %s\" % save_path)\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    word_embeddings = sess.run(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5172565 , -0.31370378, -0.70880675, ..., -0.56104946,\n",
       "         0.95771885, -0.67423892],\n",
       "       [-2.26290917,  1.69877291,  2.32762527, ...,  0.31824327,\n",
       "        -1.90039396, -1.50200367],\n",
       "       [-1.72839701,  1.62627828,  1.30277169, ...,  0.97167552,\n",
       "        -1.78930819, -1.7389096 ],\n",
       "       ..., \n",
       "       [-1.84970224,  2.36827064,  3.14763093, ...,  2.6933074 ,\n",
       "        -1.65665507, -2.73144078],\n",
       "       [-4.36480808,  3.27600956,  4.73933983, ...,  3.43959928,\n",
       "        -4.37753296, -3.74848604],\n",
       "       [-2.83870816,  2.12040949,  1.9287231 , ...,  1.65030611,\n",
       "        -1.6661278 , -0.74331111]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def word_to_embedding(word, embeddings):\n",
    "    return embeddings[word_to_ids[word]]\n",
    "\n",
    "def words_nearest_to_vector(vector, embeddings):\n",
    "    distance = [(i, np.linalg.norm(embeddings[i] - vector)) for i in range(len(embeddings))]\n",
    "    distance = sorted(distance, key = itemgetter(1))    \n",
    "    return [id_to_words[x[0]] for x in distance[:5]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vector_man = word_to_embedding('man', word_embeddings)\n",
    "vector_male = word_to_embedding('male', word_embeddings)\n",
    "vector_woman = word_to_embedding('woman', word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['softness', 'impenetrably', 'archbishop', 'arrogant', 'mounting']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_nearest_to_vector(vector_male - vector_man + vector_woman, word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec,  glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_PATH = \"/home/canh/NLP/data/word_embeddings/\"\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove = KeyedVectors.load_word2vec_format(EMBEDDING_PATH + 'glove.6B.100d.w2vformat.txt', binary=False)\n",
    "#glove.vector_size\n",
    "#len(glove.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('female', 0.8745686411857605),\n",
       " ('women', 0.6597278714179993),\n",
       " ('adult', 0.6493057608604431),\n",
       " ('males', 0.631980299949646),\n",
       " ('pregnant', 0.6277735233306885),\n",
       " ('females', 0.6186779141426086),\n",
       " ('sex', 0.5793021321296692),\n",
       " ('heterosexual', 0.5712605714797974),\n",
       " ('girls', 0.5663195252418518),\n",
       " ('child', 0.5662558078765869)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.most_similar(positive=['woman', 'male'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_PATH + 'GoogleNews-vectors-negative300.bin.gz', \n",
    "                                             binary=True,\n",
    "                                            limit = 300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('female', 0.8426273465156555),\n",
       " ('females', 0.687408983707428),\n",
       " ('males', 0.627594530582428),\n",
       " ('Female', 0.5888205766677856),\n",
       " ('women', 0.5328131318092346),\n",
       " ('Male', 0.529462456703186),\n",
       " ('gender', 0.5162094235420227),\n",
       " ('Females', 0.4651598334312439),\n",
       " ('heterosexual', 0.4515623152256012),\n",
       " ('sexes', 0.4470379948616028)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar(positive=['woman', 'male'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
