{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"./pride_and_prejudice.txt\", \"r\") as f:\n",
    "    corpus = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = corpus[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_text(corpus):\n",
    "    # lowercase\n",
    "    corpus = sum([a.lower().split() for a in corpus if a != \"\\n\"],[])\n",
    "    corpus = [re.sub('[^A-Za-z0-9]+', '', token) for token in corpus]\n",
    "    corpus = [token for token in corpus if len(token) > 0] \n",
    "    corpus = [re.sub('^\\d+$', '_NUM_', token) for token in corpus]\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "corpus = preprocess_text(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124543"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  7044\n",
      "['13420txt', '13420zip', '15th', '18th', '1a', '1b', '1c', '1d', '1e', '1e1', '1e2', '1e3', '1e4', '1e5', '1e6', '1e7', '1e8', '1e9', '1f', '1f1', '1f2', '1f3', '1f4', '1f5', '1f6', '26th', '501c3', '_NUM_', 'a', 'abatement', 'abhorrence', 'abhorrent', 'abide', 'abiding', 'abilities', 'able', 'ablution', 'abode', 'abominable', 'abominably', 'abominate', 'abound', 'about', 'above', 'abroad', 'abrupt', 'abruptly', 'abruptness', 'absence', 'absent', 'absolute', 'absolutely', 'absurd', 'absurdities', 'absurdity', 'abundant', 'abundantly', 'abuse', 'abused', 'abusing', 'abusive', 'accede', 'acceded', 'acceding', 'accent', 'accents', 'accept', 'acceptable', 'acceptance', 'accepted', 'accepting', 'access', 'accessed', 'accessible', 'accident', 'accidental', 'accidentally', 'accompanied', 'accompany', 'accompanying', 'accomplished', 'accomplishedshe', 'accomplishment', 'accomplishments', 'accordance', 'according', 'accordingly', 'accosted', 'account', 'accounted', 'accounting', 'accounts', 'accuracy', 'accurate', 'accusation', 'accusations', 'accuse', 'accused', 'accusing', 'accustomed']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(corpus)))\n",
    "print( \"Vocabulary size: \", len(vocab))\n",
    "print(vocab[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Total words: 125000\n",
    "- Vocabulary size: 7100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word to id representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_to_ids = {vocab[i]:(i+1) for i in range(len(vocab))}\n",
    "id_to_words = {(i+1):vocab[i] for i in range(len(vocab))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6267, 4911, 2887, 2025, 4332, 4852, 319, 4797, 859, 3545, 548, 6302, 2025, 3532, 2574, 6267, 6655, 4332, 365, 367, 504, 4222, 1409, 319, 6937, 269, 4222, 5367, 6867, 7026, 3931, 1390, 3534, 2754, 3534, 572, 4388, 5390, 3534, 6516, 6267, 6251, 4332, 6267, 4911, 2887, 3735, 3274, 6937, 6302, 2025, 4388, 4369, 504, 7002, 6347, 4852, 319, 4797, 550, 3545, 548, 4756, 1521, 545, 28, 28, 2025, 28, 5213, 1521, 3587, 28, 3662, 6645, 4326, 28, 28, 3655, 2146, 974, 5658, 2114, 6665, 5953, 4332, 6302, 4911, 2887, 2025, 4852, 319, 4797, 4889, 859, 344, 6757, 4852, 319, 4797]\n"
     ]
    }
   ],
   "source": [
    "word_ids = [word_to_ids[token] for token in corpus]\n",
    "print(word_ids[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_word_pair(word_ids, C):\n",
    "    # cut corpus in batch_size\n",
    "    N = len(word_ids)    \n",
    "    M = (N-2*C) * 2*C\n",
    "    centers = [0] * M\n",
    "    targets = [0] * M\n",
    "    \n",
    "    for i in range(C, N-C):        \n",
    "        k = (i-C)*2*C + C\n",
    "        \n",
    "        for j in range(1, C + 1):            \n",
    "            centers[k - j]  = word_ids[i]\n",
    "            targets[k - j]  = word_ids[i - j]            \n",
    "            \n",
    "            centers[k + j - 1]  = word_ids[i]\n",
    "            targets[k + j - 1]  = word_ids[i + j]\n",
    "            \n",
    "    return list(zip(centers, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 1), (3, 2), (3, 4), (3, 5), (4, 2), (4, 3), (4, 5), (4, 6), (5, 3), (5, 4), (5, 6), (5, 7), (6, 4), (6, 5), (6, 7), (6, 8)]\n"
     ]
    }
   ],
   "source": [
    "example_pairs = create_word_pair([1,2,3,4,5,6,7,8], 2)\n",
    "print(example_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random \n",
    "def create_batches(word_pairs, batch_size):\n",
    "    random.shuffle(word_pairs)\n",
    "    M = len(word_pairs) // batch_size\n",
    "    if len(word_pairs) > batch_size * M:\n",
    "        M += 1\n",
    "    \n",
    "    return [word_pairs[i*batch_size:(i+1)*batch_size] for i in range(M)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(5, 4), (4, 2), (6, 7), (5, 3)],\n",
       " [(6, 5), (4, 6), (3, 4), (6, 4)],\n",
       " [(6, 8), (4, 5), (4, 3), (3, 5)],\n",
       " [(3, 2), (5, 6), (3, 1), (5, 7)]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_batches(example_pairs, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 4, 6, 5), (4, 2, 7, 3)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(*[(5, 4), (4, 2), (6, 7), (5, 3)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "- Input : batch of (id_word, id_context_word)\n",
    "- 2 embedded matrix each of size (VxD): P and Q\n",
    "-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary_size = len(vocab) + 1 # since our word id start from 1, normally there will be unknown word\n",
    "context_size = 3 # context size \n",
    "embedding_size = 20\n",
    "batch_size = 128\n",
    "\n",
    "embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0) )\n",
    "\n",
    "nce_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "\n",
    "# placeholder for input and output\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "\n",
    "train_embeddings = tf.nn.embedding_lookup(params=embeddings, ids=train_inputs)\n",
    "\n",
    "train_input_vectors = tf.nn.embedding_lookup(embeddings, train_inputs) # [batch_size, embedding_size]\n",
    "loss = tf.nn.nce_loss(\n",
    "    weights = nce_weights,  #A Tensor of shape [num_classes, dim]. The (possibly-partitioned) class embeddings.\n",
    "    biases = nce_biases, #biases: A Tensor of shape [num_classes]. The class biases.\n",
    "    labels = train_labels, # A Tensor of type int64 and shape [batch_size, num_true]. The target classes.\n",
    "    inputs = train_embeddings, # A Tensor of shape [batch_size, dim]. The forward activations of the input network.\n",
    "    num_sampled = 5,\n",
    "    num_classes = vocabulary_size, # An int. The number of possible classes.\n",
    "    num_true=1,  # An int. The number of target classes per training example.\n",
    "    sampled_values=None,\n",
    "    remove_accidental_hits=False,\n",
    "    partition_strategy='mod',\n",
    "    name='nce_loss'\n",
    ")\n",
    " \n",
    "loss = tf.reduce_mean(loss)\n",
    "    \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_word_ids = word_ids[:1000]\n",
    "sample_pairs = create_word_pair(sample_word_ids, context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_pairs = create_word_pair(word_ids, context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 , batch_number:  1000 , batch loss:  29.7847\n",
      "epoch:  0 , batch_number:  2000 , batch loss:  23.9917\n",
      "epoch:  0 , batch_number:  3000 , batch loss:  24.8392\n",
      "epoch:  0 , batch_number:  4000 , batch loss:  16.3176\n",
      "epoch:  0 , batch_number:  5000 , batch loss:  21.3982\n",
      "epoch:  1 , batch_number:  1000 , batch loss:  2.71581\n",
      "epoch:  1 , batch_number:  2000 , batch loss:  8.54126\n",
      "epoch:  1 , batch_number:  3000 , batch loss:  27.7271\n",
      "epoch:  1 , batch_number:  4000 , batch loss:  16.753\n",
      "epoch:  1 , batch_number:  5000 , batch loss:  0.664705\n",
      "epoch:  2 , batch_number:  1000 , batch loss:  19.8797\n",
      "epoch:  2 , batch_number:  2000 , batch loss:  8.65233\n",
      "epoch:  2 , batch_number:  3000 , batch loss:  1.45254\n",
      "epoch:  2 , batch_number:  4000 , batch loss:  0.7806\n",
      "epoch:  2 , batch_number:  5000 , batch loss:  15.8002\n",
      "epoch:  3 , batch_number:  1000 , batch loss:  0.230135\n",
      "epoch:  3 , batch_number:  2000 , batch loss:  1.75615\n",
      "epoch:  3 , batch_number:  3000 , batch loss:  1.1344\n",
      "epoch:  3 , batch_number:  4000 , batch loss:  11.449\n",
      "epoch:  3 , batch_number:  5000 , batch loss:  0.875858\n",
      "epoch:  4 , batch_number:  1000 , batch loss:  10.6835\n",
      "epoch:  4 , batch_number:  2000 , batch loss:  37.2003\n",
      "epoch:  4 , batch_number:  3000 , batch loss:  9.86536\n",
      "epoch:  4 , batch_number:  4000 , batch loss:  0.722416\n",
      "epoch:  4 , batch_number:  5000 , batch loss:  6.52398\n",
      "epoch:  5 , batch_number:  1000 , batch loss:  12.6982\n",
      "epoch:  5 , batch_number:  2000 , batch loss:  1.37777\n",
      "epoch:  5 , batch_number:  3000 , batch loss:  5.60708\n",
      "epoch:  5 , batch_number:  4000 , batch loss:  2.17787\n",
      "epoch:  5 , batch_number:  5000 , batch loss:  3.23923\n",
      "epoch:  6 , batch_number:  1000 , batch loss:  3.33412\n",
      "epoch:  6 , batch_number:  2000 , batch loss:  4.00369\n",
      "epoch:  6 , batch_number:  3000 , batch loss:  11.2884\n",
      "epoch:  6 , batch_number:  4000 , batch loss:  2.83487\n",
      "epoch:  6 , batch_number:  5000 , batch loss:  11.2237\n",
      "epoch:  7 , batch_number:  1000 , batch loss:  0.571257\n",
      "epoch:  7 , batch_number:  2000 , batch loss:  2.64745\n",
      "epoch:  7 , batch_number:  3000 , batch loss:  0.59885\n",
      "epoch:  7 , batch_number:  4000 , batch loss:  7.04297\n",
      "epoch:  7 , batch_number:  5000 , batch loss:  0.9062\n",
      "epoch:  8 , batch_number:  1000 , batch loss:  0.295919\n",
      "epoch:  8 , batch_number:  2000 , batch loss:  7.62661\n",
      "epoch:  8 , batch_number:  3000 , batch loss:  22.7191\n",
      "epoch:  8 , batch_number:  4000 , batch loss:  0.659763\n",
      "epoch:  8 , batch_number:  5000 , batch loss:  1.11167\n",
      "epoch:  9 , batch_number:  1000 , batch loss:  0.420333\n",
      "epoch:  9 , batch_number:  2000 , batch loss:  0.642854\n",
      "epoch:  9 , batch_number:  3000 , batch loss:  0.42938\n",
      "epoch:  9 , batch_number:  4000 , batch loss:  8.04873\n",
      "epoch:  9 , batch_number:  5000 , batch loss:  0.716237\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epoch):\n",
    "        batches = create_batches(batch_size=batch_size, word_pairs=word_pairs)\n",
    "        for (i, batch) in enumerate(batches):\n",
    "            batch_inputs, batch_labels = list(zip(*batch))\n",
    "            \n",
    "            #resaphe to [batch_size, 1] dimension\n",
    "            batch_labels = np.expand_dims(batch_labels, axis=1) \n",
    "            \n",
    "            sess.run(optimizer, feed_dict={train_inputs:batch_inputs, train_labels:batch_labels})\n",
    "            if (i+1) % 1000 == 0:\n",
    "                batch_loss = sess.run(loss,  feed_dict={train_inputs:batch_inputs, train_labels:batch_labels})\n",
    "                print(\"epoch: \", epoch, \", batch_number: \", i+1, \", batch loss: \", batch_loss)\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 2), (2, 3)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "747222"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
